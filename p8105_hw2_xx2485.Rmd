---
title: "p8105_hw2_xx2485"
author: "Xiaoni Xu"
date: "2024-09-25"
output: github_document
---

Loading needed packages:
```{r, message = FALSE}
library(tidyverse)
library(readxl)
library(ggplot2)
```

## Problem 1

**Loading and cleaning the NYC transit data; convert the entry variable from character to a logical variable** 
```{r} 
transit_df <- 
  read_csv("data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>% 
  janitor::clean_names() %>% 
  select(line, station_name, station_latitude, station_longitude, route1:route11, entry, vending, entrance_type, ada) %>% 
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))
  
```
This dataset contains the information for every entrance of every subway station of NYC. I chose a number of categories of data from the original dataset, including line, station name, station latitude/longitude, route, entry, vending, entrance type, and ADA compliance. The dimension of the dataset after cleaning is `r nrow(transit_df)` * `r ncol(transit_df)` (row * columns). Column names are renamed in a way to make them easier for coding.

The data is not quite tidy because `vending` variable is still a YES/NO character variable, the route variable is difficult to understand, and each entrance of the same station is not given a proper way to identify them.

**Count distinct stations by both station name and line**
```{r}
distinct_stations <- transit_df %>%
  distinct(station_name, line) %>%
  count()

# View the count of distinct stations
print(distinct_stations)
```
There are `r distinct_stations` distinct stations.

**Count ADA-compliant stations**
```{r}
ada_compliant_count <- transit_df %>%
  filter(ada == TRUE) %>%
  distinct(station_name, line) %>%
  count()

# View the count
print(ada_compliant_count)
```
There are `r ada_compliant_count` distinct stations that are ADA compliant.

**Calculate the proportion of station entrances / exits without vending allow entrance**
```{r}
# Filter for stations without vending and count those allowing entrance
proportion_no_vending_entry <- transit_df %>%
  filter(vending == "NO") %>%    # Select stations with no vending
  summarise(proportion = mean(entry == "TRUE"))  # Calculate the proportion allowing entry

# View the result
print(proportion_no_vending_entry)
```
The proportion of station entrances / exits without vending allow entrance is `r round((proportion_no_vending_entry * 100), 2)`%.


**Reformat the data so that route number and route name are distinct**
```{r}
transit_df_reformat <- transit_df %>%
  mutate(across(starts_with("route"), as.character)) %>%   # Convert all route columns to character
  pivot_longer(cols = starts_with("route"), # Select all columns that start with 'Route'
               names_to = "route_number",    # Create a new column for the route number
               values_to = "route_name",     # Create a new column for the route name
               values_drop_na = TRUE)        # Drop NA values
```

**Find the number of distinct stations that serve the A train**
```{r}
# Filter for stations that serve the A train and count distinct stations
distinct_a_train_stations <- transit_df_reformat %>%
  filter(route_name == "A") %>%      
  distinct(station_name, line) %>%     
  count()

# View the count of distinct A train stations
print(distinct_a_train_stations)
```
There are `r distinct_a_train_stations` distinct stations that serve the A train.

**Find the number of stations that serve the A train and are ADA compliant**
```{r}
# Filter for distinct ADA-compliant stations that serve the A train
ada_compliant_a_train_stations <- transit_df_reformat %>%
  filter(route_name == "A", ada == "TRUE") %>%  
  distinct(station_name, line) %>%             
  count()

# View the count of distinct ADA-compliant A train stations
print(ada_compliant_a_train_stations)
```
There are `r ada_compliant_a_train_stations` stations that serve the A train and are ADA compliant.


## Problem 2

**Read and clean the data**
```{r}
# Read and clean the Mr. Trash Wheel data
mr_trash_wheel_df = 
  read_excel("data/202409 Trash Wheel Collection Data.xlsx", 
             sheet = "Mr. Trash Wheel",
             col_names = TRUE) %>% # specify the sheet in the Excel file and to omit non-data entries
  janitor::clean_names() %>% # use reasonable variable names
  select(dumpster:homes_powered)%>% 
  filter(!is.na(dumpster)) %>% # Omit rows without dumpster-specific data
  mutate(sports_balls = as.integer(round(sports_balls))) %>%  # Round sports balls and convert to integer
  mutate(trash_wheel = "Mr. Trash Wheel",
         year = as.integer(year),# Add identifier
         homes_powered = (weight_tons * 500 / 30))  

# Read and clean the Professor Trash Wheel data
professor_trash_wheel_df =
  read_excel("data/202409 Trash Wheel Collection Data.xlsx",
                                        sheet = "Professor Trash Wheel",
                                        col_names = TRUE) %>%
  janitor::clean_names() %>%
  select(dumpster:homes_powered) %>%
  filter(!is.na(dumpster)) %>%
  mutate(trash_wheel = "Professor Trash Wheel",
         year = as.integer(year),
         homes_powered = (weight_tons * 500 / 30))

# Read and clean the Gwynnda Wheel data
gwynnda_df <- read_excel("data/202409 Trash Wheel Collection Data.xlsx",
                          sheet = "Gwynnda Trash Wheel",
                          col_names = TRUE) %>%
  janitor::clean_names() %>%
  select(dumpster:homes_powered) %>%
  filter(!is.na(dumpster)) %>%
  mutate(trash_wheel = "Gwynnda",
         year = as.integer(year),
         homes_powered = (weight_tons * 500 / 30))

# Combine all datasets into a single tidy dataset
combined_trash_wheel_df <- bind_rows(mr_trash_wheel_df, 
                                      professor_trash_wheel_df, 
                                      gwynnda_df)

# Display the combined dataset
print(combined_trash_wheel_df)
summary(combined_trash_wheel_df)
```

**Mr. Trash Wheel**:

```{r}
summary(mr_trash_wheel_df)
```
*  The dataset contains `r nrow(mr_trash_wheel_df)` observations and `r ncol(mr_trash_wheel_df)` categories for data. The data was collected from years `r paste(unique(mr_trash_wheel_df$year), collapse = ", ")`, with the number of observations in each year being: `r paste(table(mr_trash_wheel_df$year), collapse = ", ")`.

* The weight of trash collected by Mr. Trash Wheel dumpsters ranges from `r round(min(mr_trash_wheel_df$weight_tons, na.rm = TRUE), 2)` tons to `r round(max(mr_trash_wheel_df$weight_tons, na.rm = TRUE), 2)` tons, with an average of `r round(mean(mr_trash_wheel_df$weight_tons, na.rm = TRUE), 2)` tons per dumpster. Cumulatively, Mr. Trash Wheel has collected `r round(sum(mr_trash_wheel_df$weight_tons, na.rm = TRUE), 2)` tons of trash.

*  The volume of trash ranges from `r round(min(mr_trash_wheel_df$volume_cubic_yards, na.rm = TRUE), 2)` to `r round(max(mr_trash_wheel_df$volume_cubic_yards, na.rm = TRUE), 2)` cubic yards, averaging `r round(mean(mr_trash_wheel_df$volume_cubic_yards, na.rm = TRUE), 2)` cubic yards per dumpster, totaling approximately `r round(sum(mr_trash_wheel_df$volume_cubic_yards, na.rm = TRUE), 2)` cubic yards of trash.

*  Plastic Bottles: An average of `r round(mean(mr_trash_wheel_df$plastic_bottles, na.rm = TRUE), 0)`, bottles per dumpster, with a cumulative total of approximately `r format(sum(mr_trash_wheel_df$plastic_bottles, na.rm = TRUE), scientific = FALSE)` plastic bottles.
*  Polystyrene: An average of `r round(mean(mr_trash_wheel_df$polystyrene, na.rm = TRUE), 0)` units per dumpster, totaling approximately `r format(sum(mr_trash_wheel_df$polystyrene, na.rm = TRUE), scientific = FALSE)` units collected.
*  Cigarette Butts: Each dumpster collects an average of `r round(mean(mr_trash_wheel_df$cigarette_butts, na.rm = TRUE), 0)` butts, summing up to approximately `r format(sum(mr_trash_wheel_df$cigarette_butts, na.rm = TRUE), scientific = FALSE)` cigarette butts.
*  Glass Bottles: An average of  `r round(mean(mr_trash_wheel_df$glass_bottles, na.rm = TRUE), 0)` bottles per dumpster, resulting in a total of approximately `r format(sum(mr_trash_wheel_df$glass_bottles, na.rm = TRUE), scientific = FALSE)` glass bottles.
*  Plastic Bags: An average of `r round(mean(mr_trash_wheel_df$plastic_bags, na.rm = TRUE), 0)`, bags per dumpster, amounting to approximately `r format(sum(mr_trash_wheel_df$plastic_bags, na.rm = TRUE), scientific = FALSE)` plastic bags.
*  Wrappers: An average of `r round(mean(mr_trash_wheel_df$wrappers, na.rm = TRUE), 0)` wrappers per dumpster, with a cumulative total of approximately `r format(sum(mr_trash_wheel_df$wrappers, na.rm = TRUE), scientific = FALSE)` wrappers.
*  Sports Balls: An average of `r round(mean(mr_trash_wheel_df$sports_balls, na.rm = TRUE), 0)` balls per dumpster, with a cumulative total of approximately `r format(sum(mr_trash_wheel_df$sports_balls, na.rm = TRUE), scientific = FALSE)` balls.
*  Each dumpster powers an average of `r round(mean(mr_trash_wheel_df$homes_powered, na.rm = TRUE), 2)` homes, with a cumulative total of approximately `r round(sum(mr_trash_wheel_df$homes_powered, na.rm = TRUE, scientific = FALSE), 2)` homes powered by the collected trash.

**Professor Trash Wheel**:

```{r}
summary(professor_trash_wheel_df)
```

 
* This dataset contains `r ncol(professor_trash_wheel_df)` variables and `r nrow(professor_trash_wheel_df)` observations (dumpsters). The data was collected over the years `r paste(unique(professor_trash_wheel_df$year), collapse = ", ")`, with the number of observations in each year being: `r paste(table(professor_trash_wheel_df$year), collapse = ", ")`.

* The weight of trash collected by Professor Trash Wheel dumpsters ranges from `r round(min(professor_trash_wheel_df$weight_tons, na.rm = TRUE), 2)` tons to `r round(max(professor_trash_wheel_df$weight_tons, na.rm = TRUE), 2)` tons, with an average of `r round(mean(professor_trash_wheel_df$weight_tons, na.rm = TRUE), 2)` tons per dumpster. Cumulatively, Professor Trash Wheel has collected `r round(sum(professor_trash_wheel_df$weight_tons, na.rm = TRUE), 2)` tons of trash.

* The volume of trash collected ranges from `r round(min(professor_trash_wheel_df$volume_cubic_yards, na.rm = TRUE), 2)` cubic yards to `r round(max(professor_trash_wheel_df$volume_cubic_yards, na.rm = TRUE), 2)` cubic yards, with an average of `r round(mean(professor_trash_wheel_df$volume_cubic_yards, na.rm = TRUE), 2)` cubic yards per dumpster. In total, the dumpsters have collected `r round(sum(professor_trash_wheel_df$volume_cubic_yards, na.rm = TRUE), 2)` cubic yards of trash.

* For specific types of trash, the average number of plastic bottles collected per dumpster is `r round(mean(professor_trash_wheel_df$plastic_bottles, na.rm = TRUE), 0)`, and cumulatively, Professor Trash Wheel has collected `r format(sum(professor_trash_wheel_df$plastic_bottles, na.rm = TRUE), scientific = FALSE)` plastic bottles.

* The average number of polystyrene collected is `r round(mean(professor_trash_wheel_df$polystyrene, na.rm = TRUE), 0)`, with a total of `r format(sum(professor_trash_wheel_df$polystyrene, na.rm = TRUE), scientific = FALSE)` units collected so far.

* For cigarette butts, each dumpster collects an average of `r round(mean(professor_trash_wheel_df$cigarette_butts, na.rm = TRUE), 0)`, with a cumulative total of `r format(sum(professor_trash_wheel_df$cigarette_butts, na.rm = TRUE), scientific = FALSE)` cigarette butts collected.

* The average number of glass bottles per dumpster is `r round(mean(professor_trash_wheel_df$glass_bottles, na.rm = TRUE), 0)`, and cumulatively, Professor Trash Wheel has collected `r format(sum(professor_trash_wheel_df$glass_bottles, na.rm = TRUE), scientific = FALSE)` glass bottles.

* The average number of plastic bags collected per dumpster is `r round(mean(professor_trash_wheel_df$plastic_bags, na.rm = TRUE), 0)`, and cumulatively, the trash wheel has collected `r format(sum(professor_trash_wheel_df$plastic_bags, na.rm = TRUE), scientific = FALSE)` plastic bags.

* Each dumpster collects an average of `r round(mean(professor_trash_wheel_df$wrappers, na.rm = TRUE), 0)` wrappers, with a cumulative total of `r format(sum(professor_trash_wheel_df$wrappers, na.rm = TRUE), scientific = FALSE)` wrappers collected.

* Regarding energy, on average, each dumpster powers the equivalent of `r round(mean(professor_trash_wheel_df$homes_powered, na.rm = TRUE), 2)` homes, with a cumulative total of `r round(sum(professor_trash_wheel_df$homes_powered, na.rm = TRUE), 2)` homes powered by the collected trash.


**Gwynnda the Good Wheel of the West**:

```{r}
summary(gwynnda_df)
```

* This dataset contains `r ncol(gwynnda_df)` variables and `r nrow(gwynnda_df)` observations (dumpsters). The data was collected over the years `r paste(unique(gwynnda_df$year), collapse = ", ")`, with the number of observations in each year being: `r paste(table(gwynnda_df$year), collapse = ", ")`.

* The weight of trash collected by Gwynnda Trash Wheel dumpsters ranges from `r round(min(gwynnda_df$weight_tons, na.rm = TRUE), 2)` tons to `r round(max(gwynnda_df$weight_tons, na.rm = TRUE), 2)` tons, with an average of `r round(mean(gwynnda_df$weight_tons, na.rm = TRUE), 2)` tons per dumpster. Cumulatively, Gwynnda Trash Wheel has collected `r round(sum(gwynnda_df$weight_tons, na.rm = TRUE), 2)` tons of trash.

* The volume of trash collected ranges from `r round(min(gwynnda_df$volume_cubic_yards, na.rm = TRUE), 2)` cubic yards to `r round(max(gwynnda_df$volume_cubic_yards, na.rm = TRUE), 2)` cubic yards, with an average of `r round(mean(gwynnda_df$volume_cubic_yards, na.rm = TRUE), 2)` cubic yards per dumpster. In total, the dumpsters have collected `r round(sum(gwynnda_df$volume_cubic_yards, na.rm = TRUE), 2)` cubic yards of trash.

* For specific types of trash, the average number of plastic bottles collected per dumpster is `r round(mean(gwynnda_df$plastic_bottles, na.rm = TRUE), 0)`, and cumulatively, Gwynnda Trash Wheel has collected `r format(sum(gwynnda_df$plastic_bottles, na.rm = TRUE), scientific = FALSE)` plastic bottles.

* The average number of polystyrene collected is `r round(mean(gwynnda_df$polystyrene, na.rm = TRUE), 0)`, with a total of `r format(sum(gwynnda_df$polystyrene, na.rm = TRUE), scientific = FALSE)` units collected so far.

* For cigarette butts, each dumpster collects an average of `r round(mean(gwynnda_df$cigarette_butts, na.rm = TRUE), 0)`, with a cumulative total of `r format(sum(gwynnda_df$cigarette_butts, na.rm = TRUE), scientific = FALSE)` cigarette butts collected.

* The average number of plastic bags per dumpster is `r round(mean(gwynnda_df$plastic_bags, na.rm = TRUE), 0)`, and cumulatively, the trash wheel has collected `r format(sum(gwynnda_df$plastic_bags, na.rm = TRUE), scientific = FALSE)` plastic bags.

* Each dumpster collects an average of `r round(mean(gwynnda_df$wrappers, na.rm = TRUE), 0)` wrappers, with a cumulative total of `r format(sum(gwynnda_df$wrappers, na.rm = TRUE), scientific = FALSE)` wrappers collected.

* Regarding energy, on average, each dumpster powers the equivalent of `r round(mean(gwynnda_df$homes_powered, na.rm = TRUE), 2)` homes, with a cumulative total of `r round(sum(gwynnda_df$homes_powered, na.rm = TRUE), 2)` homes powered by the collected trash.


**Calculate the total weight of trash collected by Professor Trash Wheel**
```{r}
# Filter for Professor Trash Wheel and pull the weight column
total_weight_professor <- sum(
  combined_trash_wheel_df[combined_trash_wheel_df$trash_wheel == "Professor Trash Wheel", ] %>%
    pull(weight_tons),
  na.rm = TRUE
)

# Display the result
print(total_weight_professor)
```
The total weight of trash collected by Professor Trash Wheel is `r total_weight_professor` tons.

**Calculate the total number of cigarette butts collected by Gwynnda in June of 2022**
```{r}
# Filter for Gwynnda in June 2022 and calculate the total cigarette butts
total_cigarette_butts_gwynnda <- combined_trash_wheel_df %>%
  filter(trash_wheel == "Gwynnda" & year == 2022 & month == "June") %>%  
  pull(cigarette_butts) %>%  # Extract the cigarette_butts column
  sum(na.rm = TRUE)  # Calculate the total, ignoring NA values

# Display the result
print(total_cigarette_butts_gwynnda)
```
The total number of cigarette butts collected by Gwynnda in June of 2022 is `r format(total_cigarette_butts_gwynnda, scientific = FALSE)`.


## Problem 3

**Import all datasets of the Great British Bake Off**

The inconsistent naming conventions across the datasets is realized when checking the .csv files. Specifically, the person listed as "Joanne" in results was not matched with "Jo" & '"Jo"' in the other datasets, leading to many NA values for this personâ€™s entries if using `merge`. Thus, those names were all converted to Jo with `mutate` during the `read_csv` phase.

```{r, message = FALSE}
# Import data starting from the 4th row
results <- read_csv("data/gbb_datasets/results.csv", 
                 skip = 2) %>%   # Skip the first 3 rows
   mutate(baker = ifelse(baker == "Joanne", "Jo", baker)) # Joanne is the same person as Jo

bakes <- read_csv("data/gbb_datasets/bakes.csv") %>% 
  janitor::clean_names() %>% 
  mutate(baker = ifelse(baker == "\"Jo\"", "Jo", baker))  # cleaning all `"Jo"` entries and prepare it for the next merge 

bakers <- read_csv("data/gbb_datasets/bakers.csv") %>% 
  janitor::clean_names()

viewers <- read_csv("data/gbb_datasets/viewers.csv") %>% 
  janitor::clean_names()
```
**Preparing the data for merging**
```{r}
# Create a new column in `bakers` to extract the first name
bakers <- bakers %>%
  mutate(first_name = word(baker_name, 1))  # Extract the first word (first name) from `baker_name`

# Reshape the `viewers` dataframe to a long format
viewers_long <- viewers %>%
  pivot_longer(cols = starts_with("series_"), 
               names_to = "series", 
               names_prefix = "series_", 
               values_to = "viewers") %>%
  mutate(series = as.integer(series))
```

**Merging the dataframes**
```{r}
# Merge the two dataframes `results` and `bakes based on the three columns
merged_data <- merge(results, bakes, by = c("series", "episode", "baker"), all = TRUE) %>% # merge datasheets 'results' and 'bakes'
  left_join(bakers, by = c("baker" = "first_name", "series" = "series")) %>% # Merge `merged_data` with `bakers` based on `baker` and `series`
  left_join(viewers_long, by = c("series", "episode")) # Merge the reshaped `viewers_long` with `final_data` based on `series` and `episode`
         
```

**Check for correctness across datasets using `anti_join`**
```{r}
# Find rows in merged_data that don't match viewers_long (by series and episode)
unmatched_in_merged_1 <- 
  merged_data %>%  
  anti_join(viewers_long, by = c("series", "episode"))  # Check for missing rows in viewers_long

# Find rows in merged_data that don't match bakers (by series and first_name)
unmatched_in_merged_2 <- 
  merged_data %>%  
  anti_join(bakers, by = c("baker" = "first_name", "series"))  # Correct column name for bakers

# Find rows in merged_data that don't match bakes (by series, episode, and baker)
unmatched_in_merged_3 <- 
  merged_data %>%  
  anti_join(bakes, by = c("series", "episode", "baker"))  # Check for missing rows in bakes

# Display the results
dim(unmatched_in_merged_1)
dim(unmatched_in_merged_2)
dim(unmatched_in_merged_3)

```
The merges with viewers_long and bakers were fully successful, with no mismatches or missing data. However, the data in `bakes` is incomplete or inconsistent for 588 combinations of series, episode, and baker. 

**Organize the variables**
```{r}
# Rename `baker_name` to `baker_full_name` and rearrange columns
merged_data <- merged_data %>%
  rename(baker_full_name = baker_name) %>%  # Rename `baker_name` to `baker_full_name`
  select(series, episode, baker, baker_full_name, baker_age, baker_occupation, 
         hometown, signature_bake, show_stopper, technical, result, viewers)  # Rearrange columns
```
I rearranged the columns of the dataframe so it follows the flow of 1) episode, 2) baker's personal information, 3) baker's bake and result, and 4) the viewership.

**Export the modified dataframe as a CSV file**
```{r}
write.csv(merged_data, file = file.path("C:/Users/berns/Desktop/P8105 Data Science I/p8105_hw2_xx2485/p8105_hw2_xx2485/data/gbb_datasets", "merged_data.csv"), row.names = FALSE)
```

For the data cleaning process, I followed the steps shown below:

*  Importing CSV files: The initial datasets (results, bakes, and viewers) were imported using functions such as read_csv().

*  Skipped unnecessary rows: For some datasets, I skipped the first few rows to remove notes that were not relevant to the analysis.

*  Standardizing column names: Used functions like janitor::clean_names() to standardize column names, ensuring consistency across different datasets. Later, I changed column names using base R functions (e.g., renaming baker_name to baker_full_name)t.

*  Merging datasheets: I merged datasheets `results` and `bakes` based on the columns series, episode, and baker. `bakers` is merged based on the first names and series. The `viewers` dataset was in a wide format with columns like series_1, series_2, etc. To match the series and episode information, this dataset was reshaped into a long format using pivot_longer() to create a series and viewers column so it can also be merged into one single `merged_data` datasheet.

*  Fixing incompatible data types: I encountered data type mismatches of `series` being a character in one dataset and a numeric type in another. To resolve this, I converted the `series` column to a common data type (integer) in both datasets before merging.

The final dataset is a tidy format containing key information about bakers, their performance in various series and episodes, their demographic details, their bakes (signature, technical, show-stopper), and viewership. By combining results, bakes, and viewers, the dataset allows for comprehensive analysis, of the bakers' background, performance, and viewership engagement. The NA values and data not included in the final dataset might influence the analysis of the data.


**Create a reader-friendly table showing the star baker or winner of each episode in Seasons 5 through 10**
```{r}
# Filter the dataset for seasons 5 through 10 and episodes with a "Star Baker" or winner
star_bakers_winners <- merged_data %>%
  filter(series >= 5, series <= 10, result %in% c("STAR BAKER", "WINNER")) %>%
  select(series, episode, baker, result) %>%
  arrange(series, episode)


# Create a table plot using ggplot2
ggplot(star_bakers_winners, aes(x = factor(episode), y = factor(series))) +
  geom_tile(aes(fill = result), 
            color = "white") +  # Add a tile for each episode with color based on result
  geom_text(aes(label = baker), size = 3, color = "black") +  # Add baker names as text
  scale_fill_manual(values = c("STAR BAKER" = "#a8c97f", "WINNER" = "#f39800")) +  # Custom color scheme 
  labs(x = "Episode", y = "Series", title = "Star Baker or Winner for Each Episode (Seasons 5-10)") +
  theme_minimal() +
  theme(
    panel.grid = element_blank(),  # Remove grid lines for a cleaner look
    axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate x-axis labels for readability
    axis.text.y = element_text(face = "bold", size = 10)  # Make y-axis labels bold
  )
```

*  Season 5: Richard stands out with five wins (Episodes 2, 4, 7, 8, and 9), who is the predicted winner. However, despite Richard's multiple wins, the overall winner of Season 5 was Nancy, who only won Episodes 1 and 10.

*  Season 6: Nadiya won four episodes (Episodes 5, 8, 9, and 10), who is the predicted winner, and became the overall winner.

*  Season 7: Candice won four episodes (Episodes 2, 5, 8, and 10), who is the predicted winner, and became the overall winner.

*  Season 8: Steven and Sophie each had three wins, making predicting the winner difficult. The overall winner was Sophie.

*  Season 9: Rahul is the predictable winner, with three wins (Episodes 2, 3, and 10). He became the overall winner.

*  Season 10:  Steph is the predictable winner, with four wins (Episodes 4, 5, 6, and 8). However, the overall winner was David, who only won the final episode (Episode 10). 



**Import, clean, tidy, and organize the viewership data**
```{r}
# Show the first 10 rows of the cleaned and organized dataset
viewers_long %>%
  arrange(series, episode) %>%  # Sort by series and episode
  head(10) %>%
  print()

# Calculate the average viewership, accounting for NA values
avg_viewership_by_season <- 
  viewers_long |>
  group_by(series) |>
  summarise(
    average_viewers = mean(viewers, na.rm = TRUE),
    episodes_count = n(),
    missing_count = sum(is.na(viewers))
  )

# Visualize the data quality and averages
ggplot(avg_viewership_by_season, 
       aes(x = factor(series),
           y = average_viewers)) +
  geom_col(fill = "#84b9cb") +
  geom_text(aes(label = round(average_viewers, 2)),
            vjust = -0.5, size = 3) +
  labs(title = "Average Viewership per Season", 
       x = "Season", 
       y = "Average Viewers") +
  theme_minimal()

# Calculate the average viewership for Season 1
avg_viewership_season_1 <- viewers_long %>%
  filter(series == 1) %>%
  summarise(average_viewers = mean(viewers, na.rm = TRUE))

# Calculate the average viewership for Season 5
avg_viewership_season_5 <- viewers_long %>%
  filter(series == 5) %>%
  summarise(average_viewers = mean(viewers, na.rm = TRUE))

# Display the results

# the average viewership in Season 1
avg_viewership_season_1

# the average viewership in Season 5
avg_viewership_season_5
```

For Season 1, the average viewership was `r avg_viewership_season_1$average_viewers`; for Season 5, the average viewership was `r avg_viewership_season_5$average_viewers`.